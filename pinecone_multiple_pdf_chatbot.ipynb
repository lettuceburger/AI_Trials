{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lettuceburger/AI_Trials/blob/main/pinecone_multiple_pdf_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65999f8a",
      "metadata": {
        "id": "65999f8a"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai langchain  tiktoken pypdf unstructured[local-inference] gradio chromadb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db3b0b36",
      "metadata": {
        "id": "db3b0b36"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Pinecone, Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd8eb6d",
      "metadata": {
        "id": "9fd8eb6d"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] =\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e80c71",
      "metadata": {
        "id": "77e80c71"
      },
      "outputs": [],
      "source": [
        "#llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9af22d9",
      "metadata": {
        "id": "b9af22d9"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "pdf_loader = DirectoryLoader('C:/Users/ASUS/OneDrive/Documents/learn/', glob=\"**/*.pdf\")\n",
        "readme_loader = DirectoryLoader('C:/Users/ASUS/OneDrive/Documents/learn/', glob=\"**/*.md\")\n",
        "txt_loader = DirectoryLoader('C:/Users/ASUS/OneDrive/Documents/learn/', glob=\"**/*.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1010d19",
      "metadata": {
        "id": "d1010d19",
        "outputId": "bc277fd2-db5a-492f-9161-a45495ade2a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
            "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
            "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n"
          ]
        }
      ],
      "source": [
        "#take all the loader\n",
        "loaders = [pdf_loader, readme_loader, txt_loader]\n",
        "\n",
        "#lets create document \n",
        "documents = []\n",
        "for loader in loaders:\n",
        "    documents.extend(loader.load())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fce8808",
      "metadata": {
        "id": "6fce8808",
        "outputId": "5d601c82-dad3-43ed-f41b-3a44d82959b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 3 document(s) in your data\n",
            "There are 272002 characters in your document\n"
          ]
        }
      ],
      "source": [
        "print (f'You have {len(documents)} document(s) in your data')\n",
        "print (f'There are {len(documents[0].page_content)} characters in your document')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1e3e88",
      "metadata": {
        "id": "9b1e3e88",
        "outputId": "cf7dc097-943f-4099-f722-e078b769b53e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Created a chunk of size 1183, which is longer than the specified 1000\n",
            "Created a chunk of size 1104, which is longer than the specified 1000\n",
            "Created a chunk of size 1015, which is longer than the specified 1000\n",
            "Created a chunk of size 1124, which is longer than the specified 1000\n",
            "Created a chunk of size 1006, which is longer than the specified 1000\n",
            "Created a chunk of size 1106, which is longer than the specified 1000\n",
            "Created a chunk of size 1015, which is longer than the specified 1000\n",
            "Created a chunk of size 1213, which is longer than the specified 1000\n",
            "Created a chunk of size 1045, which is longer than the specified 1000\n",
            "Created a chunk of size 1150, which is longer than the specified 1000\n",
            "Created a chunk of size 1106, which is longer than the specified 1000\n",
            "Created a chunk of size 1194, which is longer than the specified 1000\n",
            "Created a chunk of size 1298, which is longer than the specified 1000\n",
            "Created a chunk of size 2582, which is longer than the specified 1000\n",
            "Created a chunk of size 5636, which is longer than the specified 1000\n",
            "Created a chunk of size 2133, which is longer than the specified 1000\n",
            "Created a chunk of size 1165, which is longer than the specified 1000\n",
            "Created a chunk of size 1755, which is longer than the specified 1000\n",
            "Created a chunk of size 2755, which is longer than the specified 1000\n",
            "Created a chunk of size 2178, which is longer than the specified 1000\n",
            "Created a chunk of size 1358, which is longer than the specified 1000\n",
            "Created a chunk of size 1148, which is longer than the specified 1000\n",
            "Created a chunk of size 1005, which is longer than the specified 1000\n",
            "Created a chunk of size 2419, which is longer than the specified 1000\n",
            "Created a chunk of size 1055, which is longer than the specified 1000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "855\n"
          ]
        }
      ],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=40) #chunk overlap seems to work better\n",
        "documents = text_splitter.split_documents(documents)\n",
        "print(len(documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cbbc5ae",
      "metadata": {
        "id": "1cbbc5ae",
        "outputId": "efc1b104-44d2-4074-8881-3cde9876f31d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='Global Business Driven\\n\\nHR Transformation\\n\\nThe Journey Continues\\n\\nPreface\\n\\nSee farther down the line\\n\\nYou can turn to the TV talking heads to hear opinions about the future. Or you can turn to the people in the trenches for an informed view of what really lies down the track.\\n\\nGlobal Business Driven HR Transformation is a crosscurrent of rapid changes in culture, technology, law, and leading practices. There’s no such thing as an organization that’s arrived at its destination — there are only ones that keep evolving and ones content to stand still. In our new book, Global Business Driven HR Transformation: The Journey Continues, Deloitte has mined the experience and vision of its leading field professionals to explore the many future trends that are shaping the way companies relate to their people.', metadata={'source': 'C:\\\\Users\\\\ASUS\\\\OneDrive\\\\Documents\\\\learn\\\\global-business-driven-hr-transformation.pdf'})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d2de8d",
      "metadata": {
        "id": "22d2de8d",
        "outputId": "3664f9e3-d031-4fed-c42f-8ce6f061c91f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='In the years to come, a global organization will likely be one that allows different HR practices in different places to the extent that they benefit the organization. When operations in different parts of the world each operate according to local needs, conditions, and opportunities — and when they have common systems and standards that let them share relevant information and data — then they will be the parts that make a global organization truly whole.\\n\\nGlobal Business Driven HR Transformation The Journey Continues   15\\n\\n2\\n\\n3\\n\\nSolution integration\\n\\nChange analytics\\n\\n1\\n\\nWhat is truly\\n\\nmeant by global\\n\\nSetting the Global HR\\n\\nTransformation Strategy\\n\\n2\\n\\nSolution Integration', metadata={'source': 'C:\\\\Users\\\\ASUS\\\\OneDrive\\\\Documents\\\\learn\\\\global-business-driven-hr-transformation.pdf'})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[40]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6528bb08",
      "metadata": {
        "id": "6528bb08"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32fa0e55",
      "metadata": {
        "id": "32fa0e55"
      },
      "outputs": [],
      "source": [
        "!pip install pinecone-client -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "443d0a96",
      "metadata": {
        "id": "443d0a96",
        "outputId": "997bd499-595d-4b1b-c87e-921aeddc7656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinecone API Key:········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "PINECONE_API_KEY = getpass.getpass('Pinecone API Key:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8302e014",
      "metadata": {
        "id": "8302e014",
        "outputId": "106d755c-5414-40fb-99df-d3d98515cb5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinecone Environment:········\n"
          ]
        }
      ],
      "source": [
        "PINECONE_ENV = getpass.getpass('Pinecone Environment:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f39334f8",
      "metadata": {
        "id": "f39334f8",
        "outputId": "15de4bec-19ae-4918-c194-61306285b5d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "import pinecone \n",
        "\n",
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
        "    environment=PINECONE_ENV  # next to api key in console\n",
        ")\n",
        "\n",
        "index_name = \"langchain-demo\"\n",
        "\n",
        "vectorstore = Pinecone.from_documents(documents, embeddings, index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78f59cf7",
      "metadata": {
        "id": "78f59cf7"
      },
      "outputs": [],
      "source": [
        "query = \"What is MRF?\"\n",
        "docs = vectorstore.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e97f96c0",
      "metadata": {
        "id": "e97f96c0",
        "outputId": "fbad1b33-6521-42e8-c8e5-a4082e6b4c50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs) #it went on and search on the 4 different vectors to find the similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23526a16",
      "metadata": {
        "id": "23526a16",
        "outputId": "5c70d137-ec65-4bb5-9fa0-937dcbca75ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Karvy Financial Services Ltd\n",
            "\n",
            "iii. Manpower Requirement form (MRF): All hiring requests have to be channeled through MRF. This would help us track and reconcile the category of hiring under backfill or additional hire. The idea is not be bureaucratic but to have control over the process. The MRF has to be sent to KFSL HR with pre approval from the CEO.\n",
            "\n",
            "v. Sourcing of candidates: On receipt of the MRF, KFSL HR can make use of any of the below mentioned sources as per the requirement.\n",
            "\n",
            "Job Portals – Job portals will be made available to the sourcing team. This is a very good and\n",
            "\n",
            "cost effective source for hiring of middle level positions.\n",
            "\n",
            "Internal  referral  –  All  hiring  requirements  would  be  published  for  the  information  of  all\n",
            "\n",
            "employees  of  the  company  (exceptions  can  be  made  if  the  KFSL  HR  decides  not  to  share  the\n",
            "\n",
            "requirements internally for confidential reasons). Internal referral should be made a strong source\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e544b7",
      "metadata": {
        "id": "a9e544b7"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbdb367f",
      "metadata": {
        "id": "fbdb367f"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
        "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93f33d9e",
      "metadata": {
        "id": "93f33d9e"
      },
      "outputs": [],
      "source": [
        "#chat_history = []\n",
        "#query = \"What are the grades and level in Karvy?\"\n",
        "#result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "#result[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b2a654",
      "metadata": {
        "id": "69b2a654"
      },
      "outputs": [],
      "source": [
        "#chat_history.append((query, result[\"answer\"]))\n",
        "#chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd7195c0",
      "metadata": {
        "id": "bd7195c0"
      },
      "outputs": [],
      "source": [
        "#query = \"which grade is associated with level 1 in Karvy?\"\n",
        "#result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "#result[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6199dcc",
      "metadata": {
        "id": "c6199dcc"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "import ipywidgets as widgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d3ac206",
      "metadata": {
        "id": "5d3ac206",
        "outputId": "ec9511cd-a1af-4de5-ca7f-d0e06b883edd",
        "colab": {
          "referenced_widgets": [
            "af90de752ee2471893fb010a366a7504",
            "dc7a826dd9d04f14a7868833072482bb",
            "c53b7a6cf94642e1adb61378fd0a7bdd",
            "7c8e10ae3a184d638e891fbdfd0238f9",
            "12f8b6609cac4b5db05c8513e19c91e1",
            "a564255b3d8b4d0fbae9213f5320a3c4",
            "fb7626a40f8845628fe90aeb42f7ef5a",
            "05ea61ad468b49c48a2b2f7488f0e32e",
            "f14ce138374443f19b3325f6c6f3f940",
            "c20bee8c512f4569a1c17dc56d6d0da1",
            "49f917f10fc34fb99179c9fd3d746462"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat with your data. Type 'exit' to stop\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af90de752ee2471893fb010a366a7504",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Text(value='', placeholder='Please enter your question:')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc7a826dd9d04f14a7868833072482bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> what is MRF')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c53b7a6cf94642e1adb61378fd0a7bdd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b><font color=\"Orange\">Chatbot:</font></b>  MRF stands for Manpower Requirement form. It is a for…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c8e10ae3a184d638e891fbdfd0238f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> summarize the case study: Solution integration at a Life Science client')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12f8b6609cac4b5db05c8513e19c91e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b><font color=\"Orange\">Chatbot:</font></b>  The case study is about a health care and life scienc…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a564255b3d8b4d0fbae9213f5320a3c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> what are the grades and levels in karvy')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb7626a40f8845628fe90aeb42f7ef5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b><font color=\"Orange\">Chatbot:</font></b>  The grades and levels in Karvy Financial Services Ltd…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05ea61ad468b49c48a2b2f7488f0e32e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> can you present the above data in tabular format')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f14ce138374443f19b3325f6c6f3f940",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b><font color=\"Orange\">Chatbot:</font></b>  Level 8: Vice President, Level 7: Assistant Vice Pres…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c20bee8c512f4569a1c17dc56d6d0da1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> THE PROCESS FOR CONDUCTING INQUIRY in IIMA')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49f917f10fc34fb99179c9fd3d746462",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b><font color=\"Orange\">Chatbot:</font></b>  Upon receipt of the complaint, the Chairperson/Presid…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "chat_history = []\n",
        "\n",
        "def on_submit(_):\n",
        "    query = input_box.value\n",
        "    input_box.value = \"\"\n",
        "    \n",
        "    if query.lower() == 'exit':\n",
        "        print(\"Thanks for the chat!\")\n",
        "        return\n",
        "    \n",
        "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "    chat_history.append((query, result['answer']))\n",
        "    \n",
        "    display(widgets.HTML(f'<b>User:</b> {query}'))\n",
        "    display(widgets.HTML(f'<b><font color=\"Orange\">Chatbot:</font></b> {result[\"answer\"]}'))\n",
        "\n",
        "print(\"Chat with your data. Type 'exit' to stop\")\n",
        "\n",
        "input_box = widgets.Text(placeholder='Please enter your question:')\n",
        "input_box.on_submit(on_submit)\n",
        "\n",
        "display(input_box)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e37baadc",
      "metadata": {
        "id": "e37baadc"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.Button(\"Clear\")\n",
        "    \n",
        "    def respond(user_message, chat_history):\n",
        "        print(user_message)\n",
        "        print(chat_history)\n",
        "        # Get response from QA chain\n",
        "        response = qa({\"question\": user_message, \"chat_history\": chat_history})\n",
        "        # Append user message and response to chat history\n",
        "        chat_history.append((user_message, response[\"answer\"]))\n",
        "        print(chat_history)\n",
        "        return \"\", chat_history\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot], queue=False)\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75843f7d",
      "metadata": {
        "id": "75843f7d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
